5/20/25

multiprocessing uses start method "fork" on Linux (the default), which
has been working. The default on MacOS is "spawn".

But the world is moving away from fork
(https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods):

    The parent process uses os.fork() to fork the Python
    interpreter. The child process, when it begins, is effectively
    identical to the parent process. All resources of the parent are
    inherited by the child process. Note that safely forking a
    multithreaded process is problematic.

    Available on POSIX systems. Currently the default on POSIX except macOS.

    Note

    The default start method will change away from fork in Python
    3.14. Code that requires fork should explicitly specify that via
    get_context() or set_start_method().

    Changed in version 3.12: If Python is able to detect that your
    process has multiple threads, the os.fork() function that this
    start method calls internally will raise a DeprecationWarning. Use
    a different start method. See the os.fork() documentation for
    further explanation.

This gives one reason why fork is not preferred on MacOS:
https://forums.macrumors.com/threads/python-m1-and-multiprocessing.2292927/

So:

- I've been developing on Python 3.10 lately.

- 3.13 is out, and 3.14 is in pre-release.

- The multiprocessing default start method will be spawn. It is pretty
  much required on Mac now.

*** Need to get spawn working.

But spawn doesn't just work:

M 0.34.2 jao@loon ~/git/marcel/notes$ gen 3
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/jao/git/marcel/marcel/main.py", line 517, in <module>
    main()
  File "/home/jao/git/marcel/marcel/main.py", line 501, in main
    main_interactive_run(locations)
  File "/home/jao/git/marcel/marcel/main.py", line 425, in main_interactive_run
    main.run()
  File "/home/jao/git/marcel/marcel/main.py", line 339, in run
    self.parse_and_run_command(self.input)
  File "/home/jao/git/marcel/marcel/main.py", line 205, in parse_and_run_command
    self.execute_command(command, pipeline)
  File "/home/jao/git/marcel/marcel/main.py", line 324, in execute_command
    self.job_control.create_job(command)
  File "/home/jao/git/marcel/marcel/job.py", line 251, in create_job
    job = Job(self.env, command)
  File "/home/jao/git/marcel/marcel/job.py", line 80, in __init__
    self.start_process()
  File "/home/jao/git/marcel/marcel/job.py", line 181, in start_process
    self.process.start()
  File "/usr/lib/python3.10/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/lib/python3.10/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/usr/lib/python3.10/multiprocessing/context.py", line 288, in _Popen
    return Popen(process_obj)
  File "/usr/lib/python3.10/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/usr/lib/python3.10/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/lib/python3.10/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/usr/lib/python3.10/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
AttributeError: Can't pickle local object 'Job.start_process.<locals>.run_command_in_child'


A Job's work is done by the run_command_in_child function inside
Job.start_process. A multiprocessing.Process receives its state via
pickling, and a non-top-level function can't be pickled.

Making run_command_in_child top-level is easy.

But then:

M 0.34.2 jao@loon ~/git/marcel/notes$ gen 3
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/jao/git/marcel/marcel/main.py", line 517, in <module>
    main()
  File "/home/jao/git/marcel/marcel/main.py", line 501, in main
    main_interactive_run(locations)
  File "/home/jao/git/marcel/marcel/main.py", line 425, in main_interactive_run
    main.run()
  File "/home/jao/git/marcel/marcel/main.py", line 339, in run
    self.parse_and_run_command(self.input)
  File "/home/jao/git/marcel/marcel/main.py", line 205, in parse_and_run_command
    self.execute_command(command, pipeline)
  File "/home/jao/git/marcel/marcel/main.py", line 324, in execute_command
    self.job_control.create_job(command)
  File "/home/jao/git/marcel/marcel/job.py", line 230, in create_job
    job = Job(self.env, command)
  File "/home/jao/git/marcel/marcel/job.py", line 80, in __init__
    self.start_process()
  File "/home/jao/git/marcel/marcel/job.py", line 160, in start_process
    self.process.start()
  File "/usr/lib/python3.10/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/lib/python3.10/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/usr/lib/python3.10/multiprocessing/context.py", line 288, in _Popen
    return Popen(process_obj)
  File "/usr/lib/python3.10/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/usr/lib/python3.10/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/lib/python3.10/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/usr/lib/python3.10/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
AttributeError: Can't pickle local object 'Environment.initialize_namespace.<locals>.<lambda>'

The job is being pickled, and Job.env (an Environment) isn't easily
serialized.

This is NOT EASY TO FIX because run_command_in_child calls
command.execute(job.env, ...). Command execution cannot be done
without an env.

So one of the following need to happen:

1. Make Environment pickleable.

2. Somehow get the spawned process to create its own Environment. 

These actually aren't all that different. The spawned command may need
something unpickleable in the Environment. So what has to be done is
to make the default Environment stuff (placed there my marcel, not a
user) pickleable.

......................................................................

So why does remote execution work? Remote uses farcel, run by
subprocess.Popen. An Environment is NOT sent, farcel constructs its
own environment. Remote execution CANNOT rely on things like env
vars. Seems to be OK. But locally, most things are done via Jobs,
which will need access to env vars, for example.

......................................................................

The Environment.initialize_namespace problem is due to the lambda,
associated with "parse_args". Is that the only problem? Commenting it
out.

M 0.34.2 jao@loon ~/git/marcel/notes$ gen 3
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/jao/git/marcel/marcel/main.py", line 517, in <module>
    main()
  File "/home/jao/git/marcel/marcel/main.py", line 501, in main
    main_interactive_run(locations)
  File "/home/jao/git/marcel/marcel/main.py", line 425, in main_interactive_run
    main.run()
  File "/home/jao/git/marcel/marcel/main.py", line 339, in run
    self.parse_and_run_command(self.input)
  File "/home/jao/git/marcel/marcel/main.py", line 205, in parse_and_run_command
    self.execute_command(command, pipeline)
  File "/home/jao/git/marcel/marcel/main.py", line 324, in execute_command
    self.job_control.create_job(command)
  File "/home/jao/git/marcel/marcel/job.py", line 230, in create_job
    job = Job(self.env, command)
  File "/home/jao/git/marcel/marcel/job.py", line 80, in __init__
    self.start_process()
  File "/home/jao/git/marcel/marcel/job.py", line 160, in start_process
    self.process.start()
  File "/usr/lib/python3.10/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/lib/python3.10/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/usr/lib/python3.10/multiprocessing/context.py", line 288, in _Popen
    return Popen(process_obj)
  File "/usr/lib/python3.10/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/usr/lib/python3.10/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/lib/python3.10/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/usr/lib/python3.10/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <function <lambda> at 0x78f5e1826830>: attribute lookup <lambda> on __main__ failed


Why is __main__ being pickled?!


*** pathos.multiprocess (https://github.com/uqfoundation/multiprocess)
    replaces pickling in multiprocessing with dill.

It is not at all clear how to use pathos. Seems to not duplicate the
entire multiprocessing API, so you need to use
multiprocessing.set_start_method, for example. Then, it isn't clear
how to get Pipes and Processes.

----------------------------------------------------------------------

5/21/25

marcel.util.PickleDebugger is really good at finding pickling errors!
Lots of problems turning up in Environment and subclasses. Need to
better organize keys from initalize_namespace methods.

Environment:

    MARCEL_VERSION
    HOME
    PWD
    DIRS
    USER
    HOST
    parse_args

EnvironmentScript:

    WORKSPACE
    PROMPT
    BOLD
    ITALIC
    COLOR_SCHEME
    Color
    pos
    o

EnvironmentAPI, EnvironmentInteractive: initialize_namespace not
overridden.

- Move parse_args to EnvironmentScript

- Move PROMPT, BOLD, ITALIC, COLOR_SCHEME, Color to
  EnvironmentInteractive

- __getstate__: Clear parse_args, PROMPT, BOLD, ITALIC, COLOR_SCHEME, Color

More troublemakers in Environment.namespace:

- o: function
- builtin functions: Added by EnvironmentScript.initialize_namespace
- __builtins__: Added when startup script is execed.
- Stuff from startup.py

Fix:

- __getstate__ removes o, builtins, __builtins__.

- Track the vars introduced by startup.py and remove those.

- __setstate__ needs to restore everything.

----------------------------------------------------------------------

5/23/25

EnvironmentInteractive setup:

main_interactive_run

    EnvironmentInteractive.create(locations, workspace, trace)

        env = EnvironmentInteractive.__init__(...)
            EnvironmentScript.__init__(...)
                Environment.__init__(NestedNamespace())

        env.initialize_namespace
            EnvironmentScript.initialize_namespace()
                Environment.initialize()
                    add MARCEL_VERSION, HOME, PWD, DIRS, USER, HOST, parse_args
                add WORKSPACE, pos, o
                add symbols from marcel.builtin
            add PROMPT, BOLD, ITALIC, COLOR_SCHEME, Color

startup file read after Env has been created

----------------------------------------------------------------------

5/25/25

Before spawning a job's process, the Env needs to be stripped down,
and then put back together.

Inputs to an Env (from above):

- Execing the startup script.

- Symbols added by Env*.initialize_namespace().

- __builtins__: Added on exec of startup script

- marcel.builtin symbols

- Side effects of marcel commands. THESE HAVE TO BE SERIALIZED across
  spawning process.


----------------------------------------------------------------------

5/26/25

Current order of building Environment namespace:

1. Environment.initialize_namespace

     MARCEL_VERSION: str
     HOME: str
     PWD: str
     DIRS: [str]
     USER: str
     HOST: str
     parse_args: function

2. EnvironmentScript.initialize_namespace

     WORKSPACE: str
     pos: function
     o: function
     <contents of marcel.builtin>: functions

3. EnvironmentInteractive.initialize_namespace

     PROMPT: [including functions]
     BOLD: int
     ITALIC: int
     COLOR_SCHEME: ColorScheme, {str: int}
     Color: ints

4. MainScript.read_config

     __builtins__
     <runs startup.py>

5. <run_on_startup calls in startup.py>

6. Side effects of marcel commands

----------------------------------------------------------------------

5/31/25

Env __getstate__

- Environment: del parse_args, pos

- EnvironmentScript: not present

- EnvironmentInteractive: del PROMPT, BOLD, COLOR_SCHEME, Color

Fixes:

- Add ES.__getstate__, del pos, o. (pos is currently taken care of by
  Environment.)

- Add __setstate__

----------------------------------------------------------------------

6/6/25

Imports show up in two ways:

1) Python import in startup.py

2) import op

- Env.imports saved as part of workspace (in env.pickle. Keys are
  namespace, imports, compilables)

- Env.imports populated by Env.import_module, which is used by env op.

- Env.compilables: list of var names whose values are compilable (can
  be regenerated).

Generalization:

- Unify all env values: class EnvValue.

- EnvValues has save/restore methods. Different handling based on value type:

  - Simple types: save/restore preserve the value.

  - module: Use Import

  - function: Use Compilable

  - Should handle reservoirs/picklefiles too.

- Once this is done, env.pickle can be simplified:

  - Drop imports and compilables

  - Just store namespaces directly, (don't need a namespaces key).

  - env setvar save arg no longer needed (set to False for imports)

- THIS IS A MIGRATION! On-disk workspace format is changing.

----------------------------------------------------------------------

6/7/25

Imports don't fit this model well. The problem is that "import math *"
imports all the symbols from the math module, with each symbol
becoming an env var, e.g. math.pi. math also brings in functions, and
those would get handled individually by this EnvValue mechanism. That
won't work because such functions don't necessarily have source, and
so can't be Compilables.

*** Imports still need special handling by Env. Track what comes in
    with an import, and remove it when pickling.

It gets messy, since math.pi could overwrite a var created by the
user. So it's order dependent. Yech.

So the EnvValue (or whatever is tracking env values) needs to know if
the symbol came from an import. Messy.

----------------------------------------------------------------------

6/8/25

Idea:

- The EnvValue idea is still workable.

- But there are now more special cases:

  - simple type like int str, [int].

  - Compilable (marcel-defined function, pipeline)

  - Python function (which comes from a module)

  - Python module

  - Reservoir

Repeated imports of the same module (e.g. due to multiple Python
functions from the same module) should be harmless. The importlib doc
sounds like modules are cached once imported.

----------------------------------------------------------------------

6/9/25

Okay, it's start to work (with an empty startup script)! Getting a
prompt!

But: Env __getstate__, which is called when starting a job's process,
IS DESTRUCTIVE. Need to apply destruction to a copy of the Env. I
believe that has proven problematic before.

----------------------------------------------------------------------

6/16/25

__getstate__ now returns a copy.

New problem: namespace using EnvValue wrappers is too finicky. Need to
wrap/unwrap the values sometimes. E.g.

- wrap them for ordinary usage
- unwrap them before execing startup script
- But also when set_prompt runs

It's too messy and error-prone. Seems unworkable.

Alternative: Overhaul NestedNamespace

- Fixing the transmission problem in Environment isn't adequate. There
  are values in outer scopes of the NestedNamespace that need the same
  treatment. Env just addresses the innermost scope.

- NestedNamespace is extremely unintuitive. I don't understand it
  every time I look at it.

......................................................................

NestedNamespace overhaul:

Still needs to be a subclass of dict, for use in evaling functions.

The use of NestedNamespace copies stored in scopes is what is so
confusing. Instead:

- NestedNamespace manages Scopes.

- Each scope has a set of params (pipeline args) and a dict of EnvValues.

- NN.__setitem__ maintains the parent dict and the dict of EnvValues.

--------------------------------------------------------------------------------

6/20/26

CHANGES STASHED while migration work is in progress.

----------------------------------------------------------------------

6/29/25

Environment state needs to be reduced on __getstate__, reconstructed in __setstate__.

What's in Environment:

    Environment:
    
        Fields:
            - namespace
            - directory_state
            - _op_modules
            - var_handler
            - trace
    
        Namespace contents (set in initialize_namespace):
            - MARCEL_VERSION
            - HOME
            - PWD
            - DIRS
            - USER
            - HOST
            - parse_args
    
        Other:
            - dir_state: change current dir
    
    EnvironmentScript:
    
        Fields:
            - locations
            - startup_vars
            - current_op
            - workspace
            - imports
            - directory_state
    
        Namespace contents:
            - WORKSPACE
            - pos
            - o
            - run_on_startup
            - STARTUP_SCRIPTS
    
        Other:
            - dir_state: change current dir
    
    EnvironmentInteractive:
    
        Fields:
            - config_path
            - reader
            - next_command
    
        Namespace:
            - PROMPT
            - set_prompt
            - BOLD
            - ITALIC
            - COLOR_SCHEME
            - Color

    EnvironmentAPI:

        Fields:
            - directory_state


Handling across serialization:

    Fields:
        - E     namespace: transmit
        - E     directory_state: clear/recreate
        - E     _op_modules: clear
        - E     var_handler: clear/recreate
        - E     trace: clear/recreate
        - E     locations: clear/recreate
        - E     current_op: clear
        - E     workspace: clear/reopen
        - imports: Should be handled via namespace
        - EI       reader: clear
        - EI       next_command: clear

----------------------------------------------------------------------

7/2/25

Env/NN serialization is still confused. They point at each other,
which is a problem: If Env cleanup (removing non-serializable stuff in
__getstate__) happens after NN.__getstate__, then because NN points to
Env, the entire Env is serialized. Need to break the cycle.

Scopes also point to Env, that has to be cleared up too.

----------------------------------------------------------------------

7/8/25

Imports

......................................................................

Import statement -> module_name, symbol, name

"import foo *" -> 
    - module_name foo
    - symbol *
    - name None

Turn this into multiple imports, one for each imported symbol
(filtering out symbol names starting with _ I think).

......................................................................

startup.py

For each symbol defined after execution of startup.py:

- If it came from an import, create an Import for it. 

- Need to research how to distinguish imported symbols from those
  defined in startup.py. I think the latter have __module__ = None.

----------------------------------------------------------------------

7/9/25

Import metadata:

Functions and modules need to be re-imported when an Env is
reconstructed (in __setstate__, following transmission).

- Function: Compilable functions (from marcel source) are already
  taken care of. Imported functions need to record enough info to
  re-import.

- Module: Note the module, re-import.

----------------------------------------------------------------------

7/10/25

Survey of import-related code:

- EnvironmentScript.Import and .imports: used to track what is
  imported via the import op. The contents of ES.imports is stored in
  env.pickle.

- nestednamespace.Function: A function from an imported module, or a
  Python builtin function.

- nestednamespace.Module: A module.

- ES.import_module(): Called from Import.run (import op)

----------------------------------------------------------------------

7/24/25


Env lifecycle:

- Marcel startup

  main()
      main_interactive_run()
          env_and_main()
              EnvironmentInteractive.create()
                  EI.__init__()
                      NestedNamespace(self)
                  env.initialize_namespace()
              MainInteractive(..., env, ...)
                  MainScript(..., env, ...)
                      if workspace exists: env.restore_persistent_state_from_workspace()
                      self.read_startup()

- Marcel restart (startup.py edit, ws open)

- Pickling (for job execution, sudo, remote, ws persistence)

- Unpickling

Things to pickle (see EnvValue.wrap):

- Python function

- Module

- Marcel function (Compilable)

- Pipeline (Compilable)

- other (Simple)

Sources:

- startup.py python

- startup.py run_on_startup()

- marcel builtin

- Hardwired in env.py

- User actions

  - assign op
  - store
  - enter/exit args scope
  - enter/exit pipeline
  - commands that change current dir
  - import op

......................................................................

Main & Env

- Why does env_and_main exist? The creation of Environment and Main
are interleaved, e.g. due to
env.restore_persistent_state_from_workspace(), and read_startup().
(Main.read_startup uses self ONLY to refer to self.env.)

- Why is main.workspace needed? Could get it from env.

*** It should be possible to untangle Main and Env setup.

----------------------------------------------------------------------

7/27/25

The handling of Workspace.persistent_state is fussy and
fragile. Playing with this, I wondered: Why does Environment own the
namespace? Environment is an older concept, Workspaces were added
recently, so NestedNamespace stayed in Environment. But Workspace owns
env.pickle which, when unpickled, goes to the namespace, owned by
Environment.

*** Idea: 

- Move NestedNamespace to Workspace. 

- save/restore env state now belongs to Workspace entirely.

......................................................................

Before that, need to fix save/restore persistent state to get things
working.

- Workspace has persistent_state field

- Workspace.close saves env.persistent_state() which computes
  {namespace:, imports:, compilables:}

- Workspace.read_environment loads env.pickle and assigns
  self.persistent_state.

- ES.restore_persistent_state_from_workspace() uses
  Workspace.persistent_state to restore namespace (saved_vars),
  compilables, imports.

----------------------------------------------------------------------

7/29/25

Workspace open, restore are a mess and failing. Survey:

- ES.restore_persistent_state_from_workspace called from:

  - WsOpen.run
  - ES.__init__

- Workspace.open called from:

  - WsNew.run
  - WsOpen.run
  - WsClose.run (calls Workspace.default().open())
  - ES.restore_persistent_state_from_workspace

- ES.restore_persistent_state_from_workspace:

  - Assumes workspace.persistent_state has been set (done by
    Workspace.open())

  - Initializes env with imports, compilables, namespace from ws
    persistent_state.

  - Set current dir

*** Move call of ES.restore... from __init__ to initialize_namespace.

......................................................................

Okay, test are running again.

If namespace is moving to env:

- Env is still the one object made available everywhere, e.g. to
  pipelines and ops. So just as a practical matter, workspaces cannot
  completely replace env.

- Env owns:

  - var API and VarHandler, var tracking

  - locations

  - prompts

  - convenience functions (e.g. db, cluster)

  - Marcel-specified env vars, the stuff in initialize_namespace.
  
  *** Env still sets these up, creates the NestedNamespace, passes it to
  Workspace.open()

- Workspace owns:

  - namespace

  - dir_state?

  - trace?

  - read_config() (since that is per-workspace)

*** Workspace is currently a field of EnvironmentScript. Promote to
    Environment.

----------------------------------------------------------------------

7/30/25

Unify env create functions

EI

    @staticmethod
    def create(workspace, trace=None):
        env = EnvironmentInteractive(workspace, trace)
        env.initialize_namespace()
        return env

ES

    @staticmethod
    def create(workspace, trace=None):
        env = EnvironmentScript(workspace, trace)
        env.initialize_namespace()
        return env


EA

    @staticmethod
    def create(globals):
        env = EnvironmentAPI(globals)
        env.initialize_namespace()
        return env


Combine:

    @staticmethod
    def create(env_class,
               workspace=Workspace.default(), 
               globals=None, 
               trace=None):
        env = env_class(workspace, trace)
        namespace = env.initial_namespace()
        # CLI, script usage: namespace is created by initial_namespace, globals is None
        # API: initial_namespace returns None, globals is provided by caller.
        assert (namespace is None) != (globals is None)
        namespace = namespace if namespace else globals
        env.namespace = namespace  # TODO: namespace is moving to workspace

----------------------------------------------------------------------

8/1/25

Env, env.pickle, Workspace.persistent_state


Create Workspace.persistent_state
    WsNew
        Workspace.create
            Workspace.create_on_disk
                *** write persistent state to dist with empty namespace


Write Workspace.persistent_state

    WsClose.run
        raises ReconfigureException

    main_interactive_run catches ReconfigureException
        main.shutdown(restart=True)
            MainScript.shutdown()
                self.env.workspace.close(restart)
                    write properties
                    self.write_environment(env.persistent_state())
                    clear namespace


Read Workspace.persistent_state

    WsOpen.run
        workspace.open()
        env.restore_persistent_state_from_workspace(workspace)
            workspace.open()
                self.namespace.clear()
                read properties
                read env, sets Workspace.persistent_state
            persistent_state = workspace.persistent_state
            process persistent_state imports, compilables, namespace

----------------------------------------------------------------------

8/14/25

Bug:

- Create a new workspace. Default workspace env is saved in <pid>.env.pickle.

- Shutdown

*** <pid>.env.pickle is not deleted on shutdown, it should be.

Main.shutdown: assert False

MainAPI.shutdown: 

    - return namespace

    *** WHY? Not used.

MainScript.shutdown:

    - close workspace

    - If not restart, close default workspace

    *** WHY ISN'T DEFAULT WS CLOSE NOT WORKING?

MainInteractive.shutdown

    - Job control shutdown

    - super shutdown


----------------------------------------------------------------------

8/15/25

+ Use macos branch model of VarHandlers (no VarHandlerStartup class)

+ Move VarHandler to workspace

+ env var api delegates to workspace

Okay, shutdown and VH have been cleaned up, back to the
<pid>.env.pickle bug.

----------------------------------------------------------------------

8/17/25

read_config() occurs in main, after env has been created. Delegates to
Environment.read_config(). Updates namespace.

Modifications:

- Add ReadConfig object. 

  - read()
  - run_startup()

- Run from Env.create instead, or Workspace.open 

  - AFTER adding builtins.
  - BEFORE restoring workspace state

- Instead of updating namespace, update a dict arg. That arg will be
  builtins from Env.create.

*** THIS AVOIDS PUTTING RANDOM SYMBOLS FROM STARTUP.PY INTO namespace
    scopes.

- Run startup scripts from main.

----------------------------------------------------------------------

8/18/25

VarHandler var tracking:

- immutable_vars: Cannot be modified after startup

- startup_vars: Obsolete

- save_vars: Vars that have been assigned, tracked if save=True on
  setvar call. False on calls from import_module. Used by
  VH.reservoirs(). Seems like overkill. Could find reservoirs another
  way.

- vars_read: Obsolete

- vars_written: Used to identify changed vars, transmit changes from Job.

- vars_deleted: Obsolete

add_save_vars

----------------------------------------------------------------------

8/19/25

VarHandler has been moved to workspace.py, var tracking has been
cleaned up. Somewhere along the way, default workspace creation has
been lost (or being done later than needed).

Running test_ops:

TEST = test_base.TestConsole()
    TestConsole.__init__
        TestBase.__init__
            TestConsole.reset_environment
                EnvironmentInteractive.create
                    workspace.open
                        ConfigScript.run
                            self.read_config
                                open(config_path)

Config path points to __DEFAULT__.startup.py which hasn't been
created.

----------------------------------------------------------------------

8/25/25

- Due to special handling of "builtins" (marcel symbols like PROMPT,
  symbols from startup script), the only imports to worry about are
  those from the marcel import op.

- Need to unify import handling with EnvValue.

Idea:

- Import object becomes a subclass of EnvValue.

- Each imported symbol -> (var, Import), where var is the Import's
  name. The cached value is the imported object.

- reconstitute does the import.

----------------------------------------------------------------------

8/27/25

EnvValue.wrap is broken for functions.

x = (1) is mishandled. wrap value is the int 1. The fact that it
resulted from a function evaluation doesn't matter.

x = (lambda: lambda: 1) is handled correctly. The value is lambda: 1,
and we get a Compilable Function.

----------------------------------------------------------------------

8/28/25

Code cleanup

......................................................................

env.py

  + Combine all Environment classes?

  + Do builtins values need to be lambdas? If not PermanentNamespace class not
    needed. YES, to get env into functions that need it.

  + "builtins" isn't the right word. Change NN.assign_builtin too

  + @properties: namespace, var_handler

    + There aren't many users of Environment.namespace. Should be
      possible to fix them all.

    + var_handler: Could use __getattr__ to delegate to
      self.workspace.var_handler.

......................................................................

nestednamespace.py

- Don't need to use EnvValues except for persistence. Introduce them
  in NN.__getstate__? Although, we aren't storing NN. So
  Workspace.write_environment (for example) would have to call a
  function to introduce EnvValues.

+ assign, _assign.

......................................................................

Migration

- On-disk representation must use just python types. marcel-defined
  types not allowed, or they stik around forever.

......................................................................

+ Workspace.persistent_state() returns {'namespace': persist}. Can just
  return persist instead. 

----------------------------------------------------------------------

8/30/25

Time to finally make multiprocessing via spawn work, now that
Environment has been simplified, and NestedNamespace has been
overhauled.

An Environment can now be created by calling Environment.create, and
then loading the stored namespace, (just NN.scopes[0], since the env
is dumped at the top level of a pipeline). So:

Environment.__getstate__:

    - Return {'usage': env.usage, 
              'workspace': workspace name,
              'namespace': scopes[0]}

Environment.__setstate__:

    - Environment.create, passing workspace name, usage

    - Add namespace contents

----------------------------------------------------------------------

9/1/25

This blows up, running with spawn multiprocessing:

     ws -n test
     gen 3

The problem is that the parent is using a non-default workspace, and
the child is too. The child fails when it finds that the workspace is
already locked.

There are two possible fixes here:

1) Parent closes the workspace while the child is using it.

2) The parent and child can use the workspace at the same time.

Either should work assuming that the command runs in foreground. But
what if the command is put into the background? The child is running,
using the workspace, and the parent goes on to run other commands,
also using the workspace. This is exactly what locking is intended to
prevent!

In other words, this is a collision of features between non-default
workspaces and background jobs. For default workspaces, there is no
exclusion, but things could get ugly. What if the child is appending
to a file, and the parent does something conflicting with the file?

On the other hand, bash has exactly the same problem. I can have a
background job writing to a file, while in the foreground I use the
file in incompatible ways.

----------------------------------------------------------------------

9/9/25

fork N (| ... |) doesn't work. The problem is that the pipeline needs
to be serialized, and ForkManager relies on the customize_pipeline
mechanism, in which, the customizer is a method, and methods can't be
pickled. And this mechanism is used extensively!

From PickleDebugger:

START: (<class 'marcel.core.PipelineMarcel'>) 0x79f6b8195f90
    *** Caught PicklingError or TypeError on Pipeline(gen 3): cannot pickle '_io.TextIOWrapper' object
    pipeline_arg: (<class 'marcel.core.PipelineExecutable'>) 0x79f6b81942b0
    customize_pipeline: (<class 'method'>) 0x79f6b998be00
        *** Caught PicklingError or TypeError on <bound method ForkWorker.customize_pipeline of <marcel.op.forkmanager.ForkWorker object at 0x79f6b8196980>>: cannot pickle '_io.TextIOWrapper' object
        TERMINAL: (<class 'method'>) 0x79f6b998be00
    pipeline: (<class 'marcel.core.PipelineExecutable'>) 0x79f6b8194c70
    params: (<class 'list'>) 0x79f6b81b4b80
    scope: (<class 'dict'>) 0x79f6b85c8e00
END Pipeline(gen 3)


But union, join (for example) work! Because the command (e.g. gen 3 |
union (| gen 3 |)) doesn't contain a customizer.

Nesting of ForkManager customize_pipeline functions:

Fork.setup
    ForkManager() called with default customize_pipeline: lambda env, pipeline, thread_id: pipeline
    FM.setup()
        ForkWorker.__init__()
            Pipeline.create(..., self.customize_pipeline)    customize_pipeline is a method

Fork.run
    FM.start_process
        
----------------------------------------------------------------------

9/11/25

customize_pipeline solves this problem:

- A pipeline is created to express a command.

- Implementation of the command sometimes requires pipelines to be
  modified, or the derivation of other pipelines.

- The customization step is done at different times by PipelineMarcel,
  and PipelinePython:

  - PM: setup
  - PP: run_pipeline, prepare_to_receive

......................................................................

gen 3 | union (| gen 4 |)


union via CLI (PipelineMarcel):

- Union.setup gets PipelineExecutable

- setup calls Pipeline.create(pipeline_executable, customize_pipeline)
  -> PipelineMarcel which wraps a PipelineExecutable.

- PM.setup(): PM has the PE (pipeline_arg). Makes a copy and calls
  customize_pipeline on the copy, assigning the resulting PE to
  self.pipeline.

- After input stream to Union.receive is exhausted, we're in
  Union.flush, where the pipelines are executed by PM.run_pipeline.


union via API (PipelinePython):

- Union.setup gets OpList

- setup calls Pipeline.create(oplist, customize_pipeline) ->
  PipelinePython which wraps a PipelineExecutable.

- PP.setup(): noop

- PP.flush(): PP.run_pipeline, which calls PE.create_pipeline ->
  self. Then customize_pipeline is applied to the PE. The resulting PE
  is exeecuted.

----------------------------------------------------------------------

9/12/25

*** WHY IS CUSTOMIZE_PIPELINE TIMED DIFFERENTLY FOR PM AND PP?

I think it's because of PipelineFunctions, which are used by the api,
e.g.

    union(lambda x: ...)

The lambda is evaluated to an OpList given x, which happens during
pipeline execution, not setup.

prepare_to_receive only used by Case?! (IfBase and Tee are obsolete, I
think.)

IDEA: 

- customize_pipeline is not attached to the pipeline being modified,
  attach it to an op. (It's already a method of ops, so just remove it
  from pipelines).

- For PipelineMarcel, customize after call to op.setup

+ In order for this to work, need to register pipelines with op. Add
  Op.pipelines.

- For PipelinePython, in run_pipeline (as is done now). But pass op to
  run_pipeline so that the customizer can be located.

----------------------------------------------------------------------

9/16/25

Pipelines are getting to be messy. Lifecycle is unclear; there appear
to be redundant calls to setup; they are tracked in different places,
so sometimes you see a customized version sometimes you don't. Need to
clean this up.

......................................................................

Pipeline types:

Pipeline

    - abstraction unifying CLI and API pipelines

    - owns PipelineExecutable

    - Subclasses:

        - PipelineMarcel: Pipeline created by parsing CLI command

        - PipelinePython: Pipeline expressed in Python. There are two kinds:

            - OpList, constructed by Python code such as gen(...) | map(...)

            - PipelineFunction, wrapping a function whose evaluation
              yields an OpList.

PipelineExecutable(AbstractOp)

    - constructed by CLI parsing

    - can be executed

......................................................................

DO ALL PIPELINE USERS GO THROUGH Pipeline FOR CREATION, SETUP,
EXECUTION? Or are there situations where PipelineExecutable is used
directly?

Survey:

- Command execution

    Command.__init__: takes pipeline as arg, always a PE
        api._run_pipeline
            api.first
            api.gather
            api.run
        PipelineIterator.__init__
        PipelineMarcel.run_pipeline
        PipelinePython.run_pipeline
        MainScript.parse_and_run_command

    - API: Pipeline comes from run, gather, first. No type checking, but should be
      OpList or a Python function (that evaluates to an OpList)

    - API can also use PipelineIterator. Only use is in
      OpList.__iter__ (e.g. for x in gen(3)). CALLER creates the PE by
      calling OpList.create_pipeline(). 

      *** BYPASSES PP.

    - CLI: Passes PE. 

      *** BYPASSES PM.

- Ops

      fork: Uses ForkManager, without customization.

          CLI: PE -> PM
          API: OpList -> PP, function -> PP

      remote: Uses ForkManager, with customizer (append RunRemote,
      LabelThread ops).

          CLI: PE -> PM
          API: OpList -> PP

      difference: Append op to load contents into dict which is
      consulted on each input tuple.

      case: Redirect op appended to each pipeline (which sends
      pipeline output to the Case op's output).

      args: Append Redirect op

      join: Append op to load contents into dict which is consulted on
      each input tuple.

      sudo: No customization.

      filter: Append op to load contents into set which is consulted
      on each input tuple.

      union: Append op to propagate flush after last
      pipeline. Pipelines run from flush().

          CLI: PE -> PM
          API: OpList -> PP

      intersect: Append op to save pipeline contents. Run on first
      receive.

          CLI: PE -> PM, StringLiteral -> PM (in case of pipeline-valued var)
          API: OpList -> PP

      ForkManager: Pipeline passed in is sent to each ForkWorker. FW
      calls Pipeline.create, and passes in FW.customize_pipeline,
      which runs the FM customizer, and then appends SendToParent.


- Union does pipeline.setup, intersect does not.

- CLI: Parser should convert PE to PM

- API: _generate_op should convert Op, OpList, function to PP.

----------------------------------------------------------------------

9/18/25

Modified Parser.parse() to return a PM, not a PE, so that is creating
expected problems, getting into issues regarding the pipeline
lifecycle.

Need to get stricter about the Pipeline interface. First up: When does
Write op get appended? Currently done by
Main.parse_and_run_command. (Terminal write is not needed for API.)

- Add something like Pipeline.ensure_terminal_write_op

- PM implements it

- PP asserts False

......................................................................

PE lifecycle:

These are the steps carried out by Command.execute.

- setup:

    - Connect ops via Op.receiver.

    - Customize op pipelines

    - Op setup

- run/receive: delegate to first op

- flush: deletage to first op (which will propagate, like receive)

- cleanup: cleanup each op (don't rely on propagation)

......................................................................

Pipeline caching of PE

Pipeline

- pipeline_executable: assigned from init arg. No check for correct
  type. Pipeline.create shows that it could be something other than PE

- pipeline: Assigned in subclass, should be a PE (?)

PipelineMarcel:

- setup: pipeline_arg set to PE (taking care of
  self.pipeline_executable being a str). MAKES A COPY. self.pipeline
  is assigned the pipeline returned by customize_pipeline of the copy.

----------------------------------------------------------------------

9/19/25

Pipelines are getting better organized, but their mutability is a
problem. A pipeline stored in a var is subject to customization,
changing the var's value.

So make pipeline structure immutable:

- Get rid of op.pipelines, each op tracks their own as before.

- Replace op.customize_pipeline(pipeline) by
  customize_pipelines(). The function modifies all of the ops
  pipelines. Don't forget to setup appended ops.

- Customization is not in-place, it yields a new
  pipeline. E.g. PipelineMarcel.append -> new pipeline.

----------------------------------------------------------------------

9/22/25

Fork op

fork.setup

    - Create ForkManager, passing in self, pipeline

    - FM.setup(): Create ForkWorkers
    
fork.run

    - ForkManager.run()
      - start workers
      - wait for them to end

ForkWorker

    - init: Use MP.Pipe to get reader and writer

    - start_process

      - Create Process to execute run_pipeline_in_child, passing in pipeline
      - process.start()

    - wait:
      - Unpickle stream from process and pass to op.send()
      - join

*** How does FWs pipeline get customized?

- Each FW has its own pipeline, since they are separate processes.

----------------------------------------------------------------------

9/25/25

Fork is working (in test_ops.test_fork -- that doesn't rely on
jobs). But getting all of test_ops to work is proving fussy. Fixing
one thing breaks another. The problem appears to be details of
pipeline lifecycle, e.g. tweaking where setup is run. The lifecycle is
stilly poorly defined.

The original idea (going back to osh) was this:

- A command is a pipeline, which is a sequence of ops.

- To run that (top-level) pipeline:

  - Run setup() on each op. This prepares each op for execution.

  - Start the pipeline by calling run() on the first op. 

  - Subsequent execution is via receive/send calls.

Later additions, to PipelineExecutable:

  - Added flush(), to clear out any pending output, e.g. for Sort,
    Window ops.

  - Added cleanup(), for resource management, (e.g. Write op).

Later still, Pipeline, and subclasses PipelineMarcel and
PipelinePython, were added to abstract PE, in particular the handling
of pipeline vars.

Pipeline also adds prepare_to_receive, which is muddled -- how does it
relate to setup?

- PM.prepare_to_receive: calls PE.setup

- PM.setup: delegates to PE.setup

- PM.run_pipeline: 

  - push scope
  - executable.setup
  - executable.run
  - executable.flush
  - executable.cleanup
  - pop scope

- PP.prepare_to_receive: 
  - self.setup: noop
  - self.create_executable: noop
  - self.pipeline.setup (after customization)

- PP.setup: noop, not even PE.setup?!

- PP.run_pipeline: 

  - Calls executable.create_pipeline, which just returns self?!

  - Calls Command.execute (Pipeline.run is very recent -- haven't
    updated PP yet).

*** PM.scope seems useless

*** prepare_to_receive is used by

    - IfBase: obsolete, should be deleted
    - Tee: same
    - Case: Called from Case.setup, for each pipeline.

    prepare_to_receive should be removable.

create_pipeline:

- OpList: Used by:

  - PipelinePython.__init__ to convert OpList to PE
  - PipelineFunction: invoke function -> OpList -> PE

- PE: noop

----------------------------------------------------------------------

TODO

- Move out of MainScript init

        # Restore workspace state
        if env.workspace.exists():
            env.restore_persistent_state_from_workspace()
        else:
            self.env.workspace.does_not_exist()

- And this, can it be moved to main, right after migration?

        marcel.persistence.persistence.validate_all(self.handle_persistence_validation_errors)

  Should be possible to simplify since we no longer start in a workspace.

- Get rid of MARCEL_VERSION in env

- Are EnvValue.wrap of module, and the Module subclass needed?

- Try assignment test cases after closing and reopening a workspace.

- Test compilables and imports in
  testws.test_workspaces_and_compliables, but this comment is no
  longer true:

    # Pipelines and functions are Compilables, and require special handling, since they aren't persisted as is.
    # Instead, their source is persisted, and they are recompiled when necessary, e.g. when switching to a workspace
    # whose environment stores them.


