 Complete Python string parsing

- Complete shell string parsing (termination on things other than EOL
  and whiteapce).

- Debugging & verbosity level

----------------------------------------------------------------------

Provide a way to specify farcel config path. Maybe as a property of
the cluster.

----------------------------------------------------------------------

Tab completion:

- Tab completion for command should distinguish between first op of
  pipeline and subsequent ones. I.e., only show those ops that can be
  used in the current position within the pipeline.

----------------------------------------------------------------------

Add notion of current job, and then allow bg/fg ops to omit args.

----------------------------------------------------------------------

Would multiprocessing.Process provide better streaming than
subprocess.Popen? I.e., not waiting for op to complete.

----------------------------------------------------------------------

Should help output go through more?

----------------------------------------------------------------------

History commands

- If history were in the namespace, then edited_command wouldn't need
  special handling.

But keeping the whole history in memory is a waste of memory.

----------------------------------------------------------------------

Not all ops can be used in API, e.g. edit, help. What about fork? remote?

----------------------------------------------------------------------

API needs documentation. help? HTML?

----------------------------------------------------------------------

Exit codes? (Not just an api issue)

----------------------------------------------------------------------

window: what is supposed to happen if overlap = 0? disjoint = 0? Not tested.
test types other than int

----------------------------------------------------------------------

sudo has a zillion args. How to express this on sudo()? **kwargs?

----------------------------------------------------------------------

first constructs an Exception out of an Error.

Is it feasible to have Error carry the original exception, and reraise
it, or at least an exception of the same type? What if the Error was
remote?

----------------------------------------------------------------------

This is broken: Op.op_name() evalutes to "op".

    @staticmethod
    def check_arg(ok, arg, message):
        if not ok:
            cause = (f'Incorrect usage of {Op.op_name()}: {message}'
                     if arg is None else
                     f'Incorrect value for {arg} argument of {Op.op_name()}: {message}')
            raise marcel.exception.KillCommandException(cause)

----------------------------------------------------------------------

ps lines are often long enough to wrap. Should render_full leave off
args? If so, then provide an args method.

----------------------------------------------------------------------

What if there are two shells running at the same time -- how is
history file maintained? Probably lose updates from the first one to
exit. How should this be handled? To be safe, on exit, should read,
append, write. Atomically.

----------------------------------------------------------------------

Should jobs and commands be objects? That would allow for better
formatting.

----------------------------------------------------------------------

Should {...} work as a glob pattern? It does in bash. pathlib.Path.glob
doesn't.

ls -fr ~/git/marcel/{marcel,test} \
| select (f: f.suffix == '.py') \
| map (f: f.readlines()) \
| expand \
| red count
No qualifying paths: ['~/git/marcel/{marcel,test}']

----------------------------------------------------------------------

These commands do different things:

    ls -fr **/*.py
    ls -fr | select (f: f.suffix == '.py')

The first one avoids symlinks (or symlinks to visited directories? or
files?). The second one explores both paths.

----------------------------------------------------------------------

I keep forgetting to set pipeline's error handler. Could be done by
Pipeline.copy.

----------------------------------------------------------------------

env has paths as strings. Should be Paths.

----------------------------------------------------------------------

ls API: Need to complain if depth is other than 0 or 1

----------------------------------------------------------------------

Exhaustive type error testing in API?

----------------------------------------------------------------------

stack traces: Include them, but have print_stack check a flag to
determine if they should really be printed.

----------------------------------------------------------------------

Controlling Popen processes:

https://pymotw.com/2/subprocess/#process-groups-sessions

----------------------------------------------------------------------

Should a var hide an op by the same name?

----------------------------------------------------------------------

Can't run ssh!

----------------------------------------------------------------------

EDITOR set to host's EDITOR on startup. Which is convenient, but odd
if host value changes. How to keep the two in sync? Maybe reporting
EDITOR should always get the value of os.getenv('EDITOR')?

----------------------------------------------------------------------

Nushell uses $it for current pipeline item. Not a bad
idea. Alternative to args (without --all).

----------------------------------------------------------------------

Tab completion:

- We often know when an op ends. Can tab-complete for |, >, >>, [, ], Enter.

- We sometimes know when an arg is not a flag. Can prompt for var, (, [.

----------------------------------------------------------------------

Help needs to discuss >, >>, pipeline params.

help pipeline, examples summing filesizes are inconsistent. Unclear
that early ones compute local sum of sizes, and later ones are global.

----------------------------------------------------------------------

In api functions, why [None] instead of []? E.g., in store()

----------------------------------------------------------------------

Operator and pipeline logging, easily controllable, is needed. Can
this be done avoiding runtime penalty when logging not in use?

----------------------------------------------------------------------

ls abc.{x,y}

is not parsed properly, because the comma is its own token.

Could fix this by recognizing COMMA only in the right context. (Lexer
would need to know Parser state.)

OR: in the context that a comma is used, look for string instead, and
check that it is a comma.

----------------------------------------------------------------------

Arg checking (derived from bug 129):

Doing parse-time checking isn't sufficient. A check is also needed at
execution time, for args that need to be evaluated. The conversion
functions in ArgsParser (e.g. str_to_int) don't really allow for that
second use, as they are tied to the ArgsParser object, which isn't
available at runtime. They could *almost* be made independent of
ArgsParser, but there are some dependencies:

- self.op_name: op isn't available during arg parsing. 

- self.env

- self.current_op: For function(). Need to revisit error handling
  (which is why f.set_op(self.current_op) is called), and perhaps get
  rid of this call.

----------------------------------------------------------------------

Tab completion for env vars.

----------------------------------------------------------------------

Can threading/multiprocessing (for fork) be replaced by async io?

https://realpython.com/async-io-python/

And maybe for the bash op:

- create_subprocess_exec
- create_subprocess_shell

----------------------------------------------------------------------

Documentation for builtin functions

----------------------------------------------------------------------

Osh handled stdin:

    osh ^ ...

Need something like this for marcel.

The read op (without a FILENAME) assumes that inputs are File objects,
not a stream of text.

What about connecting stdin to input stream? If nothing shows up, no
harm. As long as we don't get stuck waiting forever.

----------------------------------------------------------------------


- Get rid of Environment.cluster(self, name). Fork should call getvar
  instead, and then ensure that the returned value is iterable. (Use
  iter() to get the iterator, or raise TypeError).


----------------------------------------------------------------------

Should allow pipeline vars for args:

    ... | args pipeline_var

M 0.13.6 jao@cheese ~/git/marcel$ gen 3 1 | args pn
Process Process-3:
Traceback (most recent call last):
  File "/usr/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jao/git/marcel/marcel/job.py", line 147, in run_command_in_child
    child_namespace_changes = command.execute(self.env)
  File "/home/jao/git/marcel/marcel/core.py", line 416, in execute
    self.pipeline.setup()
  File "/home/jao/git/marcel/marcel/core.py", line 346, in setup
    op.setup()
  File "/home/jao/git/marcel/marcel/op/args.py", line 75, in setup
    self.impl.setup()
  File "/home/jao/git/marcel/marcel/op/args.py", line 141, in setup
    self.params = op.pipeline_arg.parameters()
AttributeError: 'str' object has no attribute 'parameters'
M 0.13.6 jao@cheese ~/git/marcel$ gen 3 1 | args (pn)
Process Process-4:
Traceback (most recent call last):
  File "/usr/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jao/git/marcel/marcel/job.py", line 147, in run_command_in_child
    child_namespace_changes = command.execute(self.env)
  File "/home/jao/git/marcel/marcel/core.py", line 416, in execute
    self.pipeline.setup()
  File "/home/jao/git/marcel/marcel/core.py", line 346, in setup
    op.setup()
  File "/home/jao/git/marcel/marcel/op/args.py", line 75, in setup
    self.impl.setup()
  File "/home/jao/git/marcel/marcel/op/args.py", line 181, in setup
    self.n_params = len(self.op.pipeline_arg.__code__.co_varnames)
AttributeError: 'SourceFunction' object has no attribute '__code__'

----------------------------------------------------------------------

Add -r|--remote flag to fork. Currently there, sort of, hidden, for
use by @ syntax. (See Fork.execute_remotely()).

----------------------------------------------------------------------

Get rid of Pipeline.set_error_handler. Can the error handler be
inherited from Pipeline.copy()?

----------------------------------------------------------------------

test_ls(): See test_filename_ops. Interactive only, need API vesrion.

ls needs work on links. From bash:

-H, --dereference-command-line

    follow symbolic links listed on the command line

--dereference-command-line-symlink-to-dir

    follow each command line symbolic link that points to a directory

-L, --dereference

    when showing file information for a symbolic link, show
    information for the file the link references rather than for the
    link itself

----------------------------------------------------------------------

Args unwraps input, and tp definition has to deal with it:

tp = (| head 1 | \
        args (| x: \
            ((x,) if type(x) is not tuple else x) | \
            (*x: tuple([type(y) for y in x])) \
        |) \
     |)

Should the unwrapping by args be unconditional?

----------------------------------------------------------------------

Extend COLOR_SCHEME, adding a color for commands (i.e., user input).

----------------------------------------------------------------------

CSV handling: add options for specifying quote and delimiter. Can then
drop the distinction between CSV and TSV.

----------------------------------------------------------------------

Command history ideas:

https://www.reddit.com/r/linuxquestions/comments/12rux3h/why_are_commands_preceded_with_space_not_logged/?utm_source=share&utm_medium=ios_app&utm_name=ioscss&utm_content=1&utm_term=1

----------------------------------------------------------------------

Tab completion for executables.

----------------------------------------------------------------------

Why does marcel lose colorization?

Running grep from bash colorizes (run the command to see):

jao@cheese:~$ grep MiniM /var/log/syslog
May 28 03:00:01 cheese MiniMachete: Mounting backup
May 28 03:00:07 cheese MiniMachete: backup to localhost: Backup starting
May 28 03:35:17 cheese MiniMachete: backup to localhost: Backup finished
May 28 03:35:17 cheese MiniMachete: Unmounting backup
May 28 03:35:40 cheese MiniMachete: backup to acme: Backup starting
May 28 05:16:20 cheese MiniMachete: backup to acme: Backup finished
May 29 03:00:01 cheese MiniMachete: Mounting backup
May 29 03:00:06 cheese MiniMachete: backup to localhost: Backup starting
May 29 03:33:07 cheese MiniMachete: backup to localhost: Backup finished
May 29 03:33:07 cheese MiniMachete: Unmounting backup
May 29 03:33:28 cheese MiniMachete: backup to acme: Backup starting
May 29 04:40:02 cheese MiniMachete: backup to acme: Backup finished
May 30 03:00:01 cheese MiniMachete: Mounting backup
May 30 03:00:05 cheese MiniMachete: backup to localhost: Backup starting
May 30 03:32:38 cheese MiniMachete: backup to localhost: Backup finished
May 30 03:32:38 cheese MiniMachete: Unmounting backup
May 30 03:33:09 cheese MiniMachete: backup to acme: Backup starting
May 30 04:33:51 cheese MiniMachete: backup to acme: Backup finished

But running the same command through marcel does not.

jao@cheese:~$ marcel
M 0.16.1 jao@cheese ~$ grep MiniM /var/log/syslog
May 28 03:00:01 cheese MiniMachete: Mounting backup
May 28 03:00:07 cheese MiniMachete: backup to localhost: Backup starting
May 28 03:35:17 cheese MiniMachete: backup to localhost: Backup finished
May 28 03:35:17 cheese MiniMachete: Unmounting backup
May 28 03:35:40 cheese MiniMachete: backup to acme: Backup starting
May 28 05:16:20 cheese MiniMachete: backup to acme: Backup finished
May 29 03:00:01 cheese MiniMachete: Mounting backup
May 29 03:00:06 cheese MiniMachete: backup to localhost: Backup starting
May 29 03:33:07 cheese MiniMachete: backup to localhost: Backup finished
May 29 03:33:07 cheese MiniMachete: Unmounting backup
May 29 03:33:28 cheese MiniMachete: backup to acme: Backup starting
May 29 04:40:02 cheese MiniMachete: backup to acme: Backup finished
May 30 03:00:01 cheese MiniMachete: Mounting backup
May 30 03:00:05 cheese MiniMachete: backup to localhost: Backup starting
May 30 03:32:38 cheese MiniMachete: backup to localhost: Backup finished
May 30 03:32:38 cheese MiniMachete: Unmounting backup
May 30 03:33:09 cheese MiniMachete: backup to acme: Backup starting
May 30 04:33:51 cheese MiniMachete: backup to acme: Backup finished
M 0.16.1 jao@cheese ~$ 
            
----------------------------------------------------------------------

Fast file and string operations: for "long" strings, large files (multi GB)

https://github.com/ashvardanian/Stringzilla

----------------------------------------------------------------------

Tab completion:

After op, provide both flags and filenames.

----------------------------------------------------------------------

env no longer sent to farcel via pipeline. But what about function globals?

----------------------------------------------------------------------

Sql uses %s formatting to set vars from upstream data, e.g.

    sql 'insert into proc values(%s, %s, %s)'

Using a function:

    sql (x, y, z: 'insert into proc values((x), (y), (z))')

Or

    sql (x, y, z: f'insert into proc values({x}, {y}, {z})')

----------------------------------------------------------------------

head n (n > 0) should terminate after n. Otherwise, might need to
wait for a very long time.

But how to terminate? Throwing an exception back up to Command.execute is wrong, e.g.

    ... | ifthen (*x: ...) (| head 3 | ... |) ...

head can't terminate the entire command, but should be able to
terminate the pipeline it belongs to.

Idea:

- Throw TruncateInputStreamException

- Caught by Command.execute (or maybe Pipeline.run) and suppressed.

- Subclass receive or run can catch it and do something else. E.g. ifthen is currently:

    def receive(self, env, x):
        if self.call(env, self.predicate, *x):
            self.then.receive(env, x)
        self.send(env, x)

  Modification:

    def setup(self, env):
        ...
        self.then_active = True
        self.downstream_active = True

    def receive(self, env, x):
        if self.then_active and self.call(env, self.predicate, *x):
            try:
                self.then.receive(env, x)
            except TruncateInputStreamException:
                self.then_active = False
        if self.downstream_active:
            try:
                self.send(env, x)
            else:
                self.downstream_active = False


Hmm. This will get very complicated. Would need to identify and fix
ops having this problem, and then test head downstream of every op.

......................................................................

Idea:

- Unify pipeline execution, so that it always goes through a PipelineWrapper.

- Execute via PipelineWrapper.

- In PW, catch PipelineTerminatingException (thrown by head)

- PTE should leave the pipeline able to continue executing on further
  input. E.g.

      ... ifthen (...) (| ... | head 2 |)

  The nested pipeline receives input, throws PTE from head 2. Next
  input reruns the pipeline.

SEMANTICS ARE UNCLEAR!

What does the head 2 mean there? Is the counting on one execution of
the pipeline? Or across all of them.

----------------------------------------------------------------------

Typing:

https://mypy.readthedocs.io/en/stable/cheat_sheet_py3.html

----------------------------------------------------------------------

Fix imports to use . notation

----------------------------------------------------------------------

su doesn't work:

M 0.18.3 jao@loon /media/backup$ su
Error: ['Password: su: Authentication failure']

----------------------------------------------------------------------

Assign pipeline result to a variable. E.g. a way to run a pipeline
inside a function, something like

    x = (RUNPIPELINE(ls -fr | (f: f.size) | red +))

What about this:

    x = ls -fr | ...

This runs the pipeline and:

- converts a stream to a list
- unwraps the list if it has a single element
- if we now have a tuple, unwrap if length is 1


x = ... syntax doesn't print anything currently. Could print the value
assigned.


Parsing might be difficult. We see "x = ls". What next? It could mean
"x = ls", e.g., ls is a variable with an assigned value. Or there
could be more symbols as in "x = ls -fr | ..." 

----------------------------------------------------------------------

Store should pass on its input. This enables capture of state with
continuation of processing. See ~/hacks/condprob.m.

----------------------------------------------------------------------

condprob.m also would benefit from cross product operator. The
prepending of 0s to make join work like a cross product is a hack.

----------------------------------------------------------------------

tee is stupid. The pipelines are dead ends, so can only do anything
useful by side effect. Need a way to operate on the two
streams. Nested? Merge? Join?

----------------------------------------------------------------------

Debug mode: Collect stream after each op. Or selected ops. 

Minimal implementation:

- Have store copy input to output
- Add store ops to pipeline

Debug info is in the stored vars.

----------------------------------------------------------------------

Inline comments don't work

- FIx parser?

- Or have # be syntactic sugar for noop op which just copies input to output.

----------------------------------------------------------------------

Switch to LGPL

----------------------------------------------------------------------

pwd is a command. I think it means print working directory. So PWD as
a var name doesn't make sense, it should be WD. But bash has a PWD
also.

Does the marcel cd command change the enclosing process wd? It
probably shouldn't.

----------------------------------------------------------------------

History implementation creates a HistoryRecord for each command in
history, and then throws away probably nearly all of them if n is
set. Don't be so dumb.

----------------------------------------------------------------------

History record should include time of execution.

----------------------------------------------------------------------

Clusters and Hosts should be renderable. What else is missing that?

----------------------------------------------------------------------

Revisit checking of python and marcel version in farcel

----------------------------------------------------------------------

write -f: It would be nice if the format could be an expression
referring to vars.

----------------------------------------------------------------------

Ideas about startup: 

- Replace USER, BOLD, MARCEL_VERSION etc. by functions.

- Replace RUN_ON_STARTUP by run(...)

----------------------------------------------------------------------

help_workspace.py

Mention workspaces in top-level help

----------------------------------------------------------------------

Should restriction on select, write etc. not being permitted as first
in pipeline be dropped? Inside args, these can be OK. Maybe limit the
restriction to top-level.

----------------------------------------------------------------------

Op to write commands to script. 

----------------------------------------------------------------------

Prettyprint pipelines. Use it for rendering pipeline code.

----------------------------------------------------------------------

Should extend Exception, not BaseException: https://docs.python.org/3/library/exceptions.html#base-classes

----------------------------------------------------------------------

Shutdown of workspaces is messy. The current workspace is closed. But
also the default workspace, which is hanging around to keep the
environment alive. Maybe:

- Allow default workspaces to persist within a process, using PID
  prefixes of persistent state.

- Delete that stuff when shutting down for real.

----------------------------------------------------------------------

If Marcel cared about portability, rm, mv, etc. would have to be
implemented. Couldn’t rely on host.

----------------------------------------------------------------------

Disable workspaces for farcel

----------------------------------------------------------------------

Something approximately like:

    bash "xyz <<EOF
    ...
    EOF"

should work.

----------------------------------------------------------------------

Case can't be first in a pipeline, although it makes sense here, with
stdin piped from bash:

jao@loon:/tmp$ find test_home | marcel <<EOF
> case (f: f.name == 'g3.pickle') (| (f: 'G3.PICKLE') |) \
>                                 (| (f: f) |)
> EOF
case cannot be the first operator in a pipeline

jao@loon:/tmp$ find test_home | marcel <<EOF
case (f: f.name == 'g3.pickle') (| (f: 'G3.PICKLE') |)                                 (| (f: f) |)
EOF

----------------------------------------------------------------------

function_args_parser: Doesn't check that star is used only with last
arg. Although, python parsing catches it.

----------------------------------------------------------------------

When compact_rendering a file, need to be careful of spaces, at least
for those cases when the filename is being used in a command.

----------------------------------------------------------------------

filter has a handy --compare flag, to extract a comparison key. join
could use the same.

But it's messier for join, since a key extraction function is needed
for each join input.

----------------------------------------------------------------------

marcel < fec.m: fec.m specifies a workspace. If the workspace is already in use:

Workspace hwg is in use by another process.

The block works the other way too, if a script is running with a ws,
an interactive session using that ws can't start.

Script usage should be able to use workspaces even if there is
interactive use, but history should not be touched.

But what if the interactive user is changing the workspace in ways
that affects the script? I guess this is a Linux problem in general.

What about vars? Should changes to those persist? Probably not. So
just go with contents of startup.py?


SHOULD script usage be allowed to use a non-default workspace? 

- Definitely don't want history maintained.

- Without a ws, how would helper python functions be accessed?

- What about dbs and clusters?

- Maybe allow script invocation to specify a startup file?

----------------------------------------------------------------------

From tab completion work:

Things that might be replaced by prompt_toolkit:

- multilinereader

- Try PromptSession.complete_style

Fix:

- MultiColumnCompletionsMenu for long list of candidates?

- There's a bug in ls: ls c\*d: looks for c*d not literally "c*d". The
  escaping isn't visible to ls. Same for single and double quoting.

- What about completion of single quote: ls "

- Correct use of stringliterals for native filename ops, such as rm. E.g., does rm "a b" work?
